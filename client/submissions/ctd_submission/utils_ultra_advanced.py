# ctd_submission/utils_ultra_advanced.py
# Extracteur PDF ultra-avanc√© avec techniques combin√©es pour pr√©servation de structure

import re
import json
import os
import cv2
import numpy as np
import base64
import logging
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from collections import defaultdict, Counter
import pickle
import tempfile
from io import BytesIO

# Imports pour extraction PDF
try:
    import fitz  # PyMuPDF

    PYMUPDF_AVAILABLE = True
except ImportError:
    PYMUPDF_AVAILABLE = False

try:
    import pdfplumber

    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False

try:
    import camelot

    CAMELOT_AVAILABLE = True
except ImportError:
    CAMELOT_AVAILABLE = False

try:
    import tabula

    TABULA_AVAILABLE = True
except ImportError:
    TABULA_AVAILABLE = False

# Imports pour Computer Vision
try:
    from PIL import Image, ImageDraw, ImageFont

    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

# Imports pour Machine Learning
try:
    from sklearn.cluster import DBSCAN, KMeans
    from sklearn.metrics.pairwise import euclidean_distances
    from sklearn.preprocessing import StandardScaler

    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

# Imports pour traitement linguistique
try:
    import nltk
    from nltk.tokenize import word_tokenize, sent_tokenize
    from nltk.corpus import stopwords

    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False

logger = logging.getLogger(__name__)


@dataclass
class Position:
    """Position pr√©cise d'un √©l√©ment"""
    x: float
    y: float
    width: float
    height: float
    page: int = 0

    @property
    def x2(self) -> float:
        return self.x + self.width

    @property
    def y2(self) -> float:
        return self.y + self.height

    @property
    def center_x(self) -> float:
        return self.x + self.width / 2

    @property
    def center_y(self) -> float:
        return self.y + self.height / 2

    def overlaps(self, other: 'Position') -> bool:
        """V√©rifie si deux positions se chevauchent"""
        return not (self.x2 < other.x or other.x2 < self.x or
                    self.y2 < other.y or other.y2 < self.y)

    def distance_to(self, other: 'Position') -> float:
        """Distance entre deux positions"""
        return ((self.center_x - other.center_x) ** 2 +
                (self.center_y - other.center_y) ** 2) ** 0.5


@dataclass
class ExtractedElement:
    """√âl√©ment extrait avec m√©tadonn√©es compl√®tes"""
    type: str  # 'text', 'table', 'image', 'line', 'shape'
    content: Any
    position: Position
    style: Dict[str, Any]
    confidence: float = 0.0
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class UltraAdvancedPDFExtractor:
    """
    Extracteur PDF ultra-avanc√© combinant toutes les techniques possibles
    pour pr√©server fid√®lement la structure des documents
    """

    def __init__(self):
        self.available_extractors = []
        self.extraction_methods = {}
        # Backward-compat for callers expecting pdf_libraries attribute
        self.pdf_libraries = {
            'pymupdf': PYMUPDF_AVAILABLE,
            'pdfplumber': PDFPLUMBER_AVAILABLE,
            'camelot': CAMELOT_AVAILABLE,
            'tabula': TABULA_AVAILABLE,
        }
        self._init_extractors()

        # Configuration des seuils
        self.config = {
            'table_detection_threshold': 0.7,
            'text_clustering_eps': 15.0,
            'min_table_rows': 2,
            'min_table_cols': 2,
            'ocr_confidence_threshold': 0.6,
            'structure_validation_threshold': 0.8,
            'alignment_tolerance': 3.0,
            'merge_distance_threshold': 10.0
        }

        # Mod√®les pr√©-entra√Æn√©s (simul√©s)
        self.table_classifier = None
        self.structure_analyzer = None

    def _init_extractors(self):
        """Initialise tous les extracteurs disponibles"""
        extractors = {
            'pymupdf': PYMUPDF_AVAILABLE,
            'pdfplumber': PDFPLUMBER_AVAILABLE,
            'camelot': CAMELOT_AVAILABLE,
            'tabula': TABULA_AVAILABLE,
            'computer_vision': PIL_AVAILABLE and cv2 is not None,
            'ml_clustering': SKLEARN_AVAILABLE
        }

        for name, available in extractors.items():
            if available:
                self.available_extractors.append(name)
                logger.info(f"‚úÖ Extracteur {name} disponible")
            else:
                logger.warning(f"‚ö†Ô∏è Extracteur {name} non disponible")

    def extract_ultra_structured_content(self, file_path: str) -> Dict:
        """
        Extraction ultra-avanc√©e avec toutes les techniques combin√©es
        """
        logger.info(f"üöÄ ULTRA-EXTRACTION - D√©but pour {os.path.basename(file_path)}")

        try:
            # PHASE 1: Extraction multi-source
            raw_extractions = self._multi_source_extraction(file_path)

            # PHASE 2: Analyse computer vision
            visual_analysis = self._computer_vision_analysis(file_path)

            # PHASE 3: Clustering et analyse structurelle
            structure_analysis = self._advanced_structure_analysis(raw_extractions, visual_analysis)

            # PHASE 4: Reconstruction intelligente
            reconstructed_content = self._intelligent_reconstruction(structure_analysis)

            # PHASE 5: Validation et correction
            validated_content = self._validate_and_correct_structure(reconstructed_content)

            # PHASE 6: G√©n√©ration HTML ultra-fid√®le
            html_content = self._generate_ultra_faithful_html(validated_content)

            result = {
                'extracted': True,
                'extraction_method': 'ultra_advanced_combined',
                'confidence_score': validated_content.get('overall_confidence', 0.85),
                'text': validated_content.get('combined_text', ''),
                'html': html_content,
                'pages': validated_content.get('pages', []),
                'structure': {
                    'elements': validated_content.get('all_elements', []),
                    'tables': validated_content.get('tables', []),
                    'images': validated_content.get('images', []),
                    'text_blocks': validated_content.get('text_blocks', []),
                    'layout_analysis': validated_content.get('layout_analysis', {}),
                    'quality_metrics': validated_content.get('quality_metrics', {})
                },
                'metadata': validated_content.get('metadata', {}),
                'extraction_stats': {
                    'methods_used': len(raw_extractions),
                    'elements_found': len(validated_content.get('all_elements', [])),
                    'tables_found': len(validated_content.get('tables', [])),
                    'confidence_distribution': validated_content.get('confidence_stats', {})
                }
            }

            logger.info(f"üéâ ULTRA-EXTRACTION - Termin√©e avec succ√®s")
            return result

        except Exception as e:
            logger.error(f"‚ùå ULTRA-EXTRACTION - Erreur: {e}")
            return {'extracted': False, 'error': str(e)}

    def _multi_source_extraction(self, file_path: str) -> Dict[str, Any]:
        """Phase 1: Extraction avec toutes les sources disponibles"""
        extractions = {}

        # 1. PyMuPDF - Extraction d√©taill√©e
        if PYMUPDF_AVAILABLE:
            extractions['pymupdf'] = self._extract_with_pymupdf_advanced(file_path)

        # 2. PDFPlumber - Tables pr√©cises
        if PDFPLUMBER_AVAILABLE:
            extractions['pdfplumber'] = self._extract_with_pdfplumber_advanced(file_path)

        # 3. Camelot - Tables complexes
        if CAMELOT_AVAILABLE:
            extractions['camelot'] = self._extract_with_camelot_advanced(file_path)

        # 4. Tabula - Tables alternatives
        if TABULA_AVAILABLE:
            extractions['tabula'] = self._extract_with_tabula_advanced(file_path)

        # 5. Extraction par analyse de pixels
        extractions['pixel_analysis'] = self._extract_with_pixel_analysis(file_path)

        logger.info(f"üìä Multi-source: {len(extractions)} m√©thodes utilis√©es")
        return extractions

    def _extract_with_pymupdf_advanced(self, file_path: str) -> Dict:
        """Extraction PyMuPDF ultra-avanc√©e"""
        try:
            doc = fitz.open(file_path)
            extraction = {
                'pages': [],
                'elements': [],
                'tables': [],
                'images': [],
                'text_blocks': [],
                'fonts': [],
                'colors': [],
                'metadata': {}
            }

            for page_num in range(doc.page_count):
                page = doc.load_page(page_num)
                page_dict = page.get_text("dict")

                # Extraction ultra-d√©taill√©e
                page_data = self._process_pymupdf_page_ultra(page, page_dict, page_num + 1)
                extraction['pages'].append(page_data)

                # Agr√©ger les √©l√©ments
                extraction['elements'].extend(page_data.get('elements', []))
                extraction['tables'].extend(page_data.get('tables', []))
                extraction['images'].extend(page_data.get('images', []))
                extraction['text_blocks'].extend(page_data.get('text_blocks', []))

                # Analyser les fonts et couleurs
                self._analyze_fonts_and_colors(page_dict, extraction)

            doc.close()
            return extraction

        except Exception as e:
            logger.error(f"Erreur PyMuPDF avanc√©: {e}")
            return {}

    def _process_pymupdf_page_ultra(self, page, page_dict: Dict, page_num: int) -> Dict:
        """Traitement ultra-avanc√© d'une page PyMuPDF"""
        page_width = page.rect.width
        page_height = page.rect.height

        page_data = {
            'page_number': page_num,
            'dimensions': {'width': page_width, 'height': page_height},
            'elements': [],
            'tables': [],
            'images': [],
            'text_blocks': [],
            'lines': [],
            'shapes': []
        }

        # 1. Extraction des images avec analyse
        images = self._extract_images_ultra_detailed(page, page_num, page_width, page_height)
        page_data['images'] = images

        # 2. Extraction des formes et lignes
        shapes = self._extract_shapes_and_lines(page, page_num, page_width, page_height)
        page_data['shapes'] = shapes
        page_data['lines'] = [s for s in shapes if s['type'] == 'line']

        # 3. Extraction des blocs de texte avec clustering
        text_blocks = self._extract_text_blocks_with_clustering(page_dict, page_num, page_width, page_height)
        page_data['text_blocks'] = text_blocks

        # 4. D√©tection de tableaux par analyse g√©om√©trique
        tables = self._detect_tables_geometric_analysis(text_blocks, shapes, page_width, page_height)
        page_data['tables'] = tables

        # 5. Combiner tous les √©l√©ments
        all_elements = images + shapes + text_blocks + tables
        page_data['elements'] = sorted(all_elements, key=lambda x: (x['position']['y'], x['position']['x']))

        return page_data

    def _extract_images_ultra_detailed(self, page, page_num: int, page_width: float, page_height: float) -> List[Dict]:
        """Extraction d'images ultra-d√©taill√©e"""
        images = []

        try:
            # Obtenir toutes les images
            image_list = page.get_images(full=True)

            for img_index, img in enumerate(image_list):
                try:
                    # Obtenir les rectangles de l'image
                    image_rects = page.get_image_rects(img[0])

                    for rect_index, rect in enumerate(image_rects):
                        # Extraire l'image
                        image_data = self._extract_image_with_processing(page.parent, img[0])

                        # Analyser l'image
                        image_analysis = self._analyze_image_content(image_data)

                        image_element = {
                            'type': 'image',
                            'page': page_num,
                            'index': f"{img_index}_{rect_index}",
                            'position': {
                                'x': float(rect.x0),
                                'y': float(rect.y0),
                                'width': float(rect.width),
                                'height': float(rect.height),
                                'x_percent': (rect.x0 / page_width) * 100,
                                'y_percent': (rect.y0 / page_height) * 100,
                                'width_percent': (rect.width / page_width) * 100,
                                'height_percent': (rect.height / page_height) * 100
                            },
                            'image_data': image_data,
                            'analysis': image_analysis,
                            'confidence': 0.95
                        }

                        images.append(image_element)

                except Exception as e:
                    logger.warning(f"Erreur extraction image {img_index}: {e}")

        except Exception as e:
            logger.error(f"Erreur extraction images page {page_num}: {e}")

        return images

    def _extract_shapes_and_lines(self, page, page_num: int, page_width: float, page_height: float) -> List[Dict]:
        """Extraction des formes et lignes g√©om√©triques"""
        shapes = []

        try:
            # Obtenir les dessins vectoriels
            drawings = page.get_drawings()

            for draw_index, drawing in enumerate(drawings):
                try:
                    # drawing may be a dict-like or a tuple depending on PyMuPDF version
                    rect = None
                    stroke_color = None
                    fill_color = None
                    line_width = 1.0

                    if isinstance(drawing, dict):
                        rect = drawing.get('rect')
                        stroke = drawing.get('stroke') or {}
                        fill = drawing.get('fill') or {}
                        stroke_color = (stroke.get('color') if isinstance(stroke, dict) else None)
                        fill_color = (fill.get('color') if isinstance(fill, dict) else None)
                        line_width = (stroke.get('width', 1.0) if isinstance(stroke, dict) else 1.0)
                    elif isinstance(drawing, (list, tuple)):
                        # Best-effort unpacking for tuple-shaped drawing entries
                        # Known tuple shape: (items, color, fill, width, ...) varies by version
                        for item in drawing:
                            # try to pick first rect-like value
                            if hasattr(item, 'x0') and hasattr(item, 'y0') and hasattr(item, 'width') and hasattr(item, 'height'):
                                rect = item
                                break
                        # colors/width often not directly available; leave None/defaults

                    if not rect:
                        continue

                    # Analyser le type de forme
                    shape_type = self._classify_shape(drawing)

                    shape_element = {
                        'type': 'shape',
                        'shape_type': shape_type,
                        'page': page_num,
                        'index': draw_index,
                        'position': {
                            'x': float(rect.x0),
                            'y': float(rect.y0),
                            'width': float(rect.width),
                            'height': float(rect.height),
                            'x_percent': (rect.x0 / page_width) * 100,
                            'y_percent': (rect.y0 / page_height) * 100,
                            'width_percent': (rect.width / page_width) * 100,
                            'height_percent': (rect.height / page_height) * 100
                        },
                        'properties': {
                            'stroke_color': stroke_color,
                            'fill_color': fill_color,
                            'line_width': line_width
                        },
                        'confidence': 0.9
                    }

                    shapes.append(shape_element)

                except Exception as e:
                    logger.warning(f"Erreur traitement forme {draw_index}: {e}")

        except Exception as e:
            logger.error(f"Erreur extraction formes page {page_num}: {e}")

        return shapes

    def _extract_text_blocks_with_clustering(self, page_dict: Dict, page_num: int,
                                             page_width: float, page_height: float) -> List[Dict]:
        """Extraction de blocs de texte avec clustering ML"""
        text_blocks = []

        try:
            # Collecter tous les spans de texte
            all_spans = []
            for block in page_dict.get('blocks', []):
                if 'lines' in block:
                    for line in block['lines']:
                        for span in line.get('spans', []):
                            if span.get('text', '').strip():
                                all_spans.append(span)

            if not all_spans:
                return text_blocks

            # Clustering des spans par position et style
            clustered_spans = self._cluster_text_spans(all_spans)

            # Cr√©er des blocs de texte √† partir des clusters
            for cluster_id, spans in clustered_spans.items():
                block = self._create_text_block_from_spans(spans, page_num, page_width, page_height)
                if block:
                    text_blocks.append(block)

        except Exception as e:
            logger.error(f"Erreur clustering texte page {page_num}: {e}")

        return text_blocks

    def _cluster_text_spans(self, spans: List[Dict]) -> Dict[int, List[Dict]]:
        """Clustering des spans de texte par ML"""
        if not SKLEARN_AVAILABLE or len(spans) < 2:
            return {0: spans}

        try:
            # Pr√©parer les features pour le clustering
            features = []
            for span in spans:
                bbox = span.get('bbox', [0, 0, 0, 0])
                font_size = span.get('size', 12)

                features.append([
                    bbox[0],  # x
                    bbox[1],  # y
                    bbox[2] - bbox[0],  # width
                    bbox[3] - bbox[1],  # height
                    font_size,
                    len(span.get('text', ''))  # text length
                ])

            # Normaliser les features
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(features)

            # DBSCAN clustering
            clustering = DBSCAN(
                eps=self.config['text_clustering_eps'] / 100,  # Normalized
                min_samples=1
            ).fit(features_scaled)

            # Grouper par clusters
            clustered = defaultdict(list)
            for i, label in enumerate(clustering.labels_):
                clustered[label].append(spans[i])

            return dict(clustered)

        except Exception as e:
            logger.error(f"Erreur clustering ML: {e}")
            return {0: spans}

    def _create_text_block_from_spans(self, spans: List[Dict], page_num: int,
                                      page_width: float, page_height: float) -> Optional[Dict]:
        """Cr√©er un bloc de texte coh√©rent √† partir des spans"""
        if not spans:
            return None

        try:
            # Calculer la bounding box globale
            min_x = min(span['bbox'][0] for span in spans)
            min_y = min(span['bbox'][1] for span in spans)
            max_x = max(span['bbox'][2] for span in spans)
            max_y = max(span['bbox'][3] for span in spans)

            # Trier les spans par position (lecture naturelle)
            sorted_spans = sorted(spans, key=lambda s: (s['bbox'][1], s['bbox'][0]))

            # Combiner le texte
            combined_text = ' '.join(span['text'].strip() for span in sorted_spans if span['text'].strip())

            if not combined_text:
                return None

            # Analyser le style dominant
            style_analysis = self._analyze_dominant_style(spans)

            # Classifier le type d'√©l√©ment
            element_type = self._classify_text_element(combined_text, style_analysis)

            text_block = {
                'type': 'text',
                'element_type': element_type,
                'page': page_num,
                'text': combined_text,
                'position': {
                    'x': float(min_x),
                    'y': float(min_y),
                    'width': float(max_x - min_x),
                    'height': float(max_y - min_y),
                    'x_percent': (min_x / page_width) * 100,
                    'y_percent': (min_y / page_height) * 100,
                    'width_percent': ((max_x - min_x) / page_width) * 100,
                    'height_percent': ((max_y - min_y) / page_height) * 100
                },
                'style': style_analysis,
                'spans_count': len(spans),
                'confidence': self._calculate_text_confidence(spans, combined_text)
            }

            return text_block

        except Exception as e:
            logger.error(f"Erreur cr√©ation bloc texte: {e}")
            return None

    def _detect_tables_geometric_analysis(self, text_blocks: List[Dict], shapes: List[Dict],
                                          page_width: float, page_height: float) -> List[Dict]:
        """D√©tection avanc√©e de tableaux par analyse g√©om√©trique"""
        tables = []

        try:
            # 1. D√©tecter les grilles de lignes
            grid_tables = self._detect_tables_from_lines(shapes, page_width, page_height)
            tables.extend(grid_tables)

            # 2. D√©tecter les tableaux par alignement de texte
            alignment_tables = self._detect_tables_from_text_alignment(text_blocks, page_width, page_height)
            tables.extend(alignment_tables)

            # 3. D√©tecter les tableaux par espacement r√©gulier (si impl√©ment√©)
            if hasattr(self, '_detect_tables_from_spacing'):
                spacing_tables = self._detect_tables_from_spacing(text_blocks, page_width, page_height)
                tables.extend(spacing_tables)

            # 4. Fusionner et d√©duplicater les tableaux d√©tect√©s
            merged_tables = self._merge_overlapping_tables(tables)

            # 5. Valider et enrichir les tableaux
            validated_tables = []
            for table in merged_tables:
                validated = self._validate_and_enrich_table(table, text_blocks)
                if validated:
                    validated_tables.append(validated)

            return validated_tables

        except Exception as e:
            logger.error(f"Erreur d√©tection tableaux g√©om√©trique: {e}")
            return []

    def _detect_tables_from_lines(self, shapes: List[Dict], page_width: float, page_height: float) -> List[Dict]:
        """D√©tection de tableaux √† partir des lignes trac√©es"""
        tables = []

        try:
            # Filtrer les lignes
            lines = [s for s in shapes if s.get('shape_type') in ['line', 'rect']]

            if len(lines) < 4:  # Minimum pour un tableau
                return tables

            # Analyser les patterns de grille
            grid_patterns = self._analyze_grid_patterns(lines)

            for pattern in grid_patterns:
                if self._is_valid_table_pattern(pattern):
                    table = self._create_table_from_grid_pattern(pattern, page_width, page_height)
                    if table:
                        tables.append(table)

        except Exception as e:
            logger.error(f"Erreur d√©tection tableaux lignes: {e}")

        return tables

    def _detect_tables_from_text_alignment(self, text_blocks: List[Dict],
                                           page_width: float, page_height: float) -> List[Dict]:
        """D√©tection de tableaux par alignement de texte"""
        tables = []

        try:
            if not text_blocks:
                return tables

            # Grouper les blocs par Y (lignes potentielles)
            y_groups = self._group_text_blocks_by_y(text_blocks)

            # Analyser l'alignement des colonnes
            for y_group in y_groups:
                if len(y_group) >= self.config['min_table_cols']:
                    # D√©tecter si c'est une ligne de tableau
                    if self._is_table_row_candidate(y_group):
                        # Chercher les lignes suivantes qui s'alignent
                        table_rows = self._find_aligned_rows(y_group, y_groups)

                        if len(table_rows) >= self.config['min_table_rows']:
                            table = self._create_table_from_aligned_text(table_rows, page_width, page_height)
                            if table:
                                tables.append(table)

        except Exception as e:
            logger.error(f"Erreur d√©tection tableaux alignement: {e}")

        return tables

    def _computer_vision_analysis(self, file_path: str) -> Dict:
        """Phase 2: Analyse par computer vision"""
        if not PIL_AVAILABLE:
            return {}

        try:
            # Convertir PDF en images
            images = self._pdf_to_images(file_path)

            analysis = {
                'pages': [],
                'layout_regions': [],
                'text_regions': [],
                'table_regions': [],
                'image_regions': []
            }

            for page_num, image in enumerate(images):
                page_analysis = self._analyze_page_image(image, page_num + 1)
                analysis['pages'].append(page_analysis)

                # Agr√©ger les r√©gions
                analysis['layout_regions'].extend(page_analysis.get('layout_regions', []))
                analysis['text_regions'].extend(page_analysis.get('text_regions', []))
                analysis['table_regions'].extend(page_analysis.get('table_regions', []))
                analysis['image_regions'].extend(page_analysis.get('image_regions', []))

            return analysis

        except Exception as e:
            logger.error(f"Erreur analyse computer vision: {e}")
            return {}

    def _pdf_to_images(self, file_path: str, dpi: int = 200) -> List[np.ndarray]:
        """Convertir PDF en images pour analyse CV"""
        images = []

        try:
            if PYMUPDF_AVAILABLE:
                doc = fitz.open(file_path)

                for page_num in range(doc.page_count):
                    page = doc.load_page(page_num)

                    # Convertir en image
                    mat = fitz.Matrix(dpi / 72, dpi / 72)
                    pix = page.get_pixmap(matrix=mat)

                    # Convertir en numpy array
                    img_data = pix.tobytes("ppm")
                    nparr = np.frombuffer(img_data, np.uint8)
                    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

                    images.append(image)

                doc.close()

        except Exception as e:
            logger.error(f"Erreur conversion PDF vers images: {e}")

        return images

    def _analyze_page_image(self, image: np.ndarray, page_num: int) -> Dict:
        """Analyser une page sous forme d'image"""
        try:
            height, width = image.shape[:2]

            analysis = {
                'page_number': page_num,
                'dimensions': {'width': width, 'height': height},
                'layout_regions': [],
                'text_regions': [],
                'table_regions': [],
                'image_regions': []
            }

            # 1. D√©tection de mise en page par contours
            layout_regions = self._detect_layout_regions(image)
            analysis['layout_regions'] = layout_regions

            # 2. D√©tection de r√©gions de texte
            text_regions = self._detect_text_regions(image)
            analysis['text_regions'] = text_regions

            # 3. D√©tection de tableaux par analyse visuelle
            table_regions = self._detect_table_regions_visual(image)
            analysis['table_regions'] = table_regions

            # 4. D√©tection d'images
            image_regions = self._detect_image_regions(image)
            analysis['image_regions'] = image_regions

            return analysis

        except Exception as e:
            logger.error(f"Erreur analyse image page {page_num}: {e}")
            return {}

    def _detect_table_regions_visual(self, image: np.ndarray) -> List[Dict]:
        """D√©tection visuelle de tableaux par computer vision"""
        tables = []

        try:
            # Convertir en niveaux de gris
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

            # M√©thode 1: D√©tection de lignes horizontales et verticales
            horizontal_lines = self._detect_horizontal_lines(gray)
            vertical_lines = self._detect_vertical_lines(gray)

            # Trouver les intersections pour former des grilles
            grids = self._find_grid_intersections(horizontal_lines, vertical_lines)

            for grid in grids:
                if self._is_valid_table_grid(grid):
                    table = self._create_table_from_visual_grid(grid, image.shape)
                    tables.append(table)

            # M√©thode 2: D√©tection par template matching
            template_tables = self._detect_tables_by_template_matching(gray)
            tables.extend(template_tables)

            # M√©thode 3: D√©tection par analyse de texture
            texture_tables = self._detect_tables_by_texture_analysis(gray)
            tables.extend(texture_tables)

        except Exception as e:
            logger.error(f"Erreur d√©tection tableaux visuelle: {e}")

        return tables

    def _detect_horizontal_lines(self, gray_image: np.ndarray) -> List[Tuple]:
        """D√©tecter les lignes horizontales"""
        try:
            # Cr√©er un kernel horizontal
            horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (40, 1))

            # D√©tecter les lignes horizontales
            detected_lines = cv2.morphologyEx(gray_image, cv2.MORPH_OPEN, horizontal_kernel, iterations=2)

            # Trouver les contours
            contours, _ = cv2.findContours(detected_lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

            lines = []
            for contour in contours:
                x, y, w, h = cv2.boundingRect(contour)
                if w > 50 and h < 10:  # Filtre pour lignes horizontales
                    lines.append((x, y, x + w, y + h))

            return lines

        except Exception as e:
            logger.error(f"Erreur d√©tection lignes horizontales: {e}")
            return []

    def _detect_vertical_lines(self, gray_image: np.ndarray) -> List[Tuple]:
        """D√©tecter les lignes verticales"""
        try:
            # Cr√©er un kernel vertical
            vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 40))

            # D√©tecter les lignes verticales
            detected_lines = cv2.morphologyEx(gray_image, cv2.MORPH_OPEN, vertical_kernel, iterations=2)

            # Trouver les contours
            contours, _ = cv2.findContours(detected_lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

            lines = []
            for contour in contours:
                x, y, w, h = cv2.boundingRect(contour)
                if h > 50 and w < 10:  # Filtre pour lignes verticales
                    lines.append((x, y, x + w, y + h))

            return lines

        except Exception as e:
            logger.error(f"Erreur d√©tection lignes verticales: {e}")
            return []

    def _advanced_structure_analysis(self, raw_extractions: Dict, visual_analysis: Dict) -> Dict:
        """Phase 3: Analyse structurelle avanc√©e"""
        try:
            # Fusionner toutes les donn√©es
            all_elements = []
            all_tables = []
            all_images = []

            # Collecter depuis toutes les sources
            for method, data in raw_extractions.items():
                if isinstance(data, dict):
                    all_elements.extend(data.get('elements', []))
                    all_tables.extend(data.get('tables', []))
                    all_images.extend(data.get('images', []))

            # Ajouter les donn√©es de computer vision
            if visual_analysis:
                all_elements.extend(visual_analysis.get('layout_regions', []))
                all_tables.extend(visual_analysis.get('table_regions', []))
                all_images.extend(visual_analysis.get('image_regions', []))

            # Analyse de consensus entre m√©thodes
            consensus_analysis = self._consensus_analysis(raw_extractions, visual_analysis)

            # D√©tection des relations spatiales
            spatial_relations = self._analyze_spatial_relations(all_elements)

            # Classification ML des √©l√©ments
            ml_classification = self._ml_element_classification(all_elements)

            return {
                'all_elements': all_elements,
                'all_tables': all_tables,
                'all_images': all_images,
                'consensus': consensus_analysis,
                'spatial_relations': spatial_relations,
                'ml_classification': ml_classification
            }

        except Exception as e:
            logger.error(f"Erreur analyse structurelle: {e}")
            return {}

    def _consensus_analysis(self, raw_extractions: Dict, visual_analysis: Dict) -> Dict:
        """Analyse de consensus entre diff√©rentes m√©thodes"""
        try:
            consensus = {
                'high_confidence_elements': [],
                'conflicting_elements': [],
                'unique_elements': [],
                'agreement_score': 0.0
            }

            # Comparer les r√©sultats des diff√©rentes m√©thodes
            element_positions = defaultdict(list)

            # Collecter les positions de tous les √©l√©ments
            for method, data in raw_extractions.items():
                if isinstance(data, dict):
                    for element in data.get('elements', []):
                        pos_key = self._position_to_key(element['position'])
                        element_positions[pos_key].append((method, element))

            # Analyser le consensus
            total_elements = len(element_positions)
            agreement_count = 0

            for pos_key, elements in element_positions.items():
                if len(elements) > 1:
                    # √âl√©ment d√©tect√© par plusieurs m√©thodes
                    agreement_count += 1
                    consensus['high_confidence_elements'].append({
                        'position_key': pos_key,
                        'methods': [e[0] for e in elements],
                        'elements': [e[1] for e in elements],
                        'confidence': len(elements) / len(raw_extractions)
                    })
                else:
                    # √âl√©ment unique √† une m√©thode
                    consensus['unique_elements'].append({
                        'position_key': pos_key,
                        'method': elements[0][0],
                        'element': elements[0][1]
                    })

            consensus['agreement_score'] = agreement_count / total_elements if total_elements > 0 else 0

            return consensus

        except Exception as e:
            logger.error(f"Erreur analyse consensus: {e}")
            return {}

    def _intelligent_reconstruction(self, structure_analysis: Dict) -> Dict:
        """Phase 4: Reconstruction intelligente de la structure"""
        try:
            # √âl√©ments valid√©s par consensus
            validated_elements = self._validate_elements_by_consensus(structure_analysis)

            # Reconstruction de tableaux intelligente
            reconstructed_tables = self._reconstruct_tables_intelligently(validated_elements)

            # Reconstruction de la hi√©rarchie de contenu
            content_hierarchy = self._reconstruct_content_hierarchy(validated_elements)

            # Ordre de lecture naturel
            reading_order = self._determine_reading_order(validated_elements)

            # G√©n√©ration de texte combin√© optimis√©
            combined_text = self._generate_optimized_combined_text(validated_elements, reading_order)

            return {
                'validated_elements': validated_elements,
                'tables': reconstructed_tables,
                'content_hierarchy': content_hierarchy,
                'reading_order': reading_order,
                'combined_text': combined_text,
                'reconstruction_stats': {
                    'elements_validated': len(validated_elements),
                    'tables_reconstructed': len(reconstructed_tables),
                    'hierarchy_levels': len(content_hierarchy)
                }
            }

        except Exception as e:
            logger.error(f"Erreur reconstruction intelligente: {e}")
            return {}

    def _validate_and_correct_structure(self, reconstructed_content: Dict) -> Dict:
        """Phase 5: Validation et correction de structure"""
        try:
            validated = reconstructed_content.copy()

            # Validation des tableaux
            validated_tables = []
            for table in reconstructed_content.get('tables', []):
                validated_table = self._validate_table_structure(table)
                if validated_table:
                    validated_tables.append(validated_table)

            validated['tables'] = validated_tables

            # Correction des chevauchements
            validated['validated_elements'] = self._correct_overlapping_elements(
                reconstructed_content.get('validated_elements', [])
            )

            # Validation de la hi√©rarchie
            validated['content_hierarchy'] = self._validate_content_hierarchy(
                reconstructed_content.get('content_hierarchy', {})
            )

            # Calcul de m√©triques de qualit√©
            validated['quality_metrics'] = self._calculate_quality_metrics(validated)

            # Score de confiance global
            validated['overall_confidence'] = self._calculate_overall_confidence(validated)

            return validated

        except Exception as e:
            logger.error(f"Erreur validation structure: {e}")
            return reconstructed_content

    def _generate_ultra_faithful_html(self, validated_content: Dict) -> str:
        """Phase 6: G√©n√©ration HTML ultra-fid√®le"""
        try:
            html_parts = ['<div class="ultra-faithful-pdf-document">']

            # Style CSS int√©gr√©
            html_parts.append(self._generate_ultra_faithful_css())

            # Organiser par pages
            pages_elements = defaultdict(list)
            for element in validated_content.get('validated_elements', []):
                page_num = element.get('page', 1)
                pages_elements[page_num].append(element)

            # G√©n√©rer chaque page
            for page_num in sorted(pages_elements.keys()):
                page_html = self._generate_ultra_faithful_page_html(
                    page_num, pages_elements[page_num], validated_content
                )
                html_parts.append(page_html)

            html_parts.append('</div>')

            return '\n'.join(html_parts)

        except Exception as e:
            logger.error(f"Erreur g√©n√©ration HTML ultra-fid√®le: {e}")
            return '<div class="error">Erreur g√©n√©ration HTML</div>'

    def _generate_ultra_faithful_css(self) -> str:
        """CSS ultra-avanc√© pour pr√©servation fid√®le"""
        return '''
        <style>
        .ultra-faithful-pdf-document {
            font-family: 'Times New Roman', serif;
            background: #f8f9fa;
            padding: 20px;
        }

        .ultra-page-container {
            background: white;
            margin: 0 auto 30px auto;
            box-shadow: 0 4px 20px rgba(0,0,0,0.1);
            position: relative;
            overflow: hidden;
            border: 1px solid #ddd;
        }

        .ultra-element {
            position: absolute;
            transition: all 0.2s ease;
        }

        .ultra-element:hover {
            outline: 2px dashed #007bff;
            z-index: 10;
        }

        .ultra-text {
            font-family: inherit;
            line-height: 1.2;
            word-wrap: break-word;
            cursor: text;
        }

        .ultra-table {
            border-collapse: collapse;
            width: 100%;
            height: 100%;
        }

        .ultra-table td, .ultra-table th {
            border: 1px solid #333;
            padding: 2px 4px;
            vertical-align: top;
            font-size: inherit;
            line-height: 1.1;
        }

        .ultra-image {
            border: 1px solid #ccc;
            background: #f9f9f9;
        }

        .ultra-shape {
            border: 1px solid;
            background: transparent;
        }

        .high-confidence {
            background: rgba(40, 167, 69, 0.1);
        }

        .medium-confidence {
            background: rgba(255, 193, 7, 0.1);
        }

        .low-confidence {
            background: rgba(220, 53, 69, 0.1);
        }

        .editable-content[contenteditable="true"] {
            min-height: 1em;
            outline: none;
        }

        .editable-content[contenteditable="true"]:focus {
            background: rgba(0, 123, 255, 0.1);
            outline: 2px solid #007bff;
        }

        .table-cell-editable {
            cursor: text;
        }

        .table-cell-editable:hover {
            background: rgba(0, 123, 255, 0.05);
        }

        .table-cell-editable:focus {
            background: rgba(0, 123, 255, 0.1);
            outline: 1px solid #007bff;
        }

        @media print {
            .ultra-page-container {
                box-shadow: none;
                border: none;
                page-break-after: always;
            }

            .ultra-element:hover {
                outline: none;
            }
        }
        </style>
        '''

    def _generate_ultra_faithful_page_html(self, page_num: int, elements: List[Dict],
                                           validated_content: Dict) -> str:
        """G√©n√©rer HTML ultra-fid√®le pour une page"""
        try:
            # Calculer les dimensions de la page
            page_width = 595  # A4 default
            page_height = 842

            # Trouver les dimensions r√©elles si disponibles
            for element in elements:
                if 'page_dimensions' in element:
                    page_width = element['page_dimensions']['width']
                    page_height = element['page_dimensions']['height']
                    break

            # Calculer le ratio pour responsive
            aspect_ratio = (page_height / page_width) * 100

            html = f'''
            <div class="ultra-page-container" data-page="{page_num}" 
                 style="width: 100%; max-width: 800px; padding-bottom: {aspect_ratio:.2f}%;">
                <div class="page-label" style="position: absolute; top: -25px; left: 0; font-size: 12px; color: #666;">
                    Page {page_num}
                </div>
            '''

            # Trier les √©l√©ments par ordre de profondeur (z-index)
            sorted_elements = sorted(elements, key=lambda x: self._get_element_z_index(x))

            # G√©n√©rer chaque √©l√©ment
            for element in sorted_elements:
                element_html = self._generate_element_html_ultra_faithful(element, page_width, page_height)
                html += element_html

            html += '</div>'

            return html

        except Exception as e:
            logger.error(f"Erreur g√©n√©ration page HTML {page_num}: {e}")
            return f'<div class="error-page">Erreur page {page_num}</div>'

    def _generate_element_html_ultra_faithful(self, element: Dict, page_width: float, page_height: float) -> str:
        """G√©n√©rer HTML ultra-fid√®le pour un √©l√©ment"""
        try:
            element_type = element.get('type', 'unknown')
            position = element.get('position', {})
            confidence = element.get('confidence', 0.5)

            # Calculer les pourcentages de position
            x_percent = (position.get('x', 0) / page_width) * 100
            y_percent = (position.get('y', 0) / page_height) * 100
            width_percent = (position.get('width', 0) / page_width) * 100
            height_percent = (position.get('height', 0) / page_height) * 100

            # Classe de confiance
            confidence_class = (
                'high-confidence' if confidence > 0.8 else
                'medium-confidence' if confidence > 0.5 else
                'low-confidence'
            )

            # Style de position commun
            position_style = f'''
                left: {x_percent:.3f}%;
                top: {y_percent:.3f}%;
                width: {width_percent:.3f}%;
                height: {height_percent:.3f}%;
            '''

            # G√©n√©rer selon le type
            if element_type == 'text':
                return self._generate_text_element_html(element, position_style, confidence_class)
            elif element_type == 'table':
                return self._generate_table_element_html(element, position_style, confidence_class)
            elif element_type == 'image':
                return self._generate_image_element_html(element, position_style, confidence_class)
            elif element_type == 'shape':
                return self._generate_shape_element_html(element, position_style, confidence_class)
            else:
                return f'<!-- Unknown element type: {element_type} -->'

        except Exception as e:
            logger.error(f"Erreur g√©n√©ration √©l√©ment HTML: {e}")
            return '<!-- Error generating element -->'

    def _generate_text_element_html(self, element: Dict, position_style: str, confidence_class: str) -> str:
        """G√©n√©rer HTML pour √©l√©ment texte"""
        text = element.get('text', '')
        element_type = element.get('element_type', 'paragraph')
        style_data = element.get('style', {})
        element_id = f"text-p{element.get('page', 1)}-{element.get('index', 0)}"

        # Style de texte
        text_style = self._convert_style_to_css(style_data)

        # Tag HTML selon le type
        tag = 'h1' if element_type == 'heading' else 'p'

        return f'''
        <{tag} class="ultra-element ultra-text {confidence_class} editable-content" 
             contenteditable="true"
             data-element-id="{element_id}"
             data-original-text="{text}"
             data-confidence="{element.get('confidence', 0.5):.2f}"
             style="{position_style} {text_style}">
            {text}
        </{tag}>
        '''

    def _generate_table_element_html(self, element: Dict, position_style: str, confidence_class: str) -> str:
        """G√©n√©rer HTML pour √©l√©ment tableau"""
        table_data = element.get('data', [])
        table_id = f"table-p{element.get('page', 1)}-{element.get('index', 0)}"

        if not table_data:
            return f'<div class="ultra-element table-placeholder {confidence_class}" style="{position_style}">Tableau d√©tect√©</div>'

        html = f'''
        <div class="ultra-element {confidence_class}" style="{position_style}">
            <table class="ultra-table" data-table-id="{table_id}">
        '''

        for row_idx, row in enumerate(table_data):
            html += '<tr>'
            for col_idx, cell in enumerate(row):
                cell_content = str(cell or '').strip()
                cell_id = f"{table_id}-r{row_idx}-c{col_idx}"

                tag = 'th' if row_idx == 0 else 'td'
                html += f'''
                <{tag} class="table-cell-editable" 
                     contenteditable="true"
                     data-cell-id="{cell_id}"
                     data-row="{row_idx}"
                     data-col="{col_idx}"
                     data-original-value="{cell_content}">
                    {cell_content}
                </{tag}>
                '''
            html += '</tr>'

        html += '''
            </table>
        </div>
        '''

        return html

    def _generate_image_element_html(self, element: Dict, position_style: str, confidence_class: str) -> str:
        """G√©n√©rer HTML pour √©l√©ment image"""
        image_data = element.get('image_data')
        image_id = f"img-p{element.get('page', 1)}-{element.get('index', 0)}"

        if image_data:
            return f'''
            <div class="ultra-element ultra-image {confidence_class}" style="{position_style}">
                <img src="{image_data}" 
                     alt="Image {element.get('index', 0)}"
                     data-image-id="{image_id}"
                     style="width: 100%; height: 100%; object-fit: contain;">
            </div>
            '''
        else:
            return f'''
            <div class="ultra-element ultra-image ultra-image-placeholder {confidence_class}" 
                 style="{position_style} border: 2px dashed #ccc; display: flex; align-items: center; justify-content: center;">
                <div style="text-align: center; color: #666;">
                    <i class="fas fa-image" style="font-size: 24px; margin-bottom: 5px;"></i><br>
                    Image {element.get('index', 0)}
                </div>
            </div>
            '''

    # M√âTHODES UTILITAIRES

    def _extract_image_with_processing(self, doc, xref):
        """Extraire et traiter une image"""
        try:
            if PYMUPDF_AVAILABLE:
                fitz = self.pdf_libraries.get('pymupdf', __import__('fitz'))
                pix = fitz.Pixmap(doc, xref)

                if pix.n < 5:
                    if pix.alpha:
                        pix = fitz.Pixmap(fitz.csRGB, pix)

                    img_bytes = pix.tobytes("png")
                    img_base64 = base64.b64encode(img_bytes).decode()
                    pix = None
                    return f"data:image/png;base64,{img_base64}"

        except Exception as e:
            logger.warning(f"Erreur extraction image: {e}")

        return None

    def _analyze_image_content(self, image_data: str) -> Dict:
        """Analyser le contenu d'une image"""
        return {
            'has_text': False,
            'is_chart': False,
            'is_diagram': False,
            'complexity': 'medium'
        }

    def _classify_shape(self, drawing: Dict) -> str:
        """Classifier une forme g√©om√©trique"""
        # Logique de classification des formes
        return 'line'  # Simplifi√©

    def _analyze_dominant_style(self, spans: List[Dict]) -> Dict:
        """Analyser le style dominant d'un groupe de spans"""
        if not spans:
            return {}

        # Calculer les moyennes et modes
        avg_size = sum(s.get('size', 12) for s in spans) / len(spans)
        fonts = [s.get('font', '') for s in spans]
        most_common_font = max(set(fonts), key=fonts.count) if fonts else ''

        bold_count = sum(1 for s in spans if 'bold' in s.get('font', '').lower())
        italic_count = sum(1 for s in spans if 'italic' in s.get('font', '').lower())

        return {
            'font': most_common_font,
            'size': avg_size,
            'bold': bold_count > len(spans) / 2,
            'italic': italic_count > len(spans) / 2,
            'color': spans[0].get('color', 0) if spans else 0
        }

    def _classify_text_element(self, text: str, style: Dict) -> str:
        """Classifier un √©l√©ment de texte"""
        text_lower = text.lower().strip()

        # Titre si court, en gras ou grande taille
        if (len(text) < 100 and style.get('bold')) or style.get('size', 12) > 14:
            return 'heading'

        # Liste si commence par num√©ro ou puce
        if re.match(r'^\s*[\d\-\*\‚Ä¢]\s*', text):
            return 'list_item'

        # Table des mati√®res si contient des points de suite
        if '...' in text or re.search(r'\d+$', text):
            return 'toc_item'

        return 'paragraph'

    def _calculate_text_confidence(self, spans: List[Dict], text: str) -> float:
        """Calculer la confiance d'un bloc de texte"""
        if not spans or not text:
            return 0.0

        # Facteurs de confiance
        factors = []

        # Nombre de spans (plus = mieux)
        factors.append(min(len(spans) / 10, 1.0) * 0.3)

        # Longueur du texte
        factors.append(min(len(text) / 100, 1.0) * 0.3)

        # Coh√©rence des styles
        if len(set(s.get('font', '') for s in spans)) == 1:
            factors.append(0.4)
        else:
            factors.append(0.2)

        return sum(factors)

    def _position_to_key(self, position: Dict) -> str:
        """Convertir une position en cl√© pour comparaison"""
        x = round(position.get('x', 0) / 10) * 10
        y = round(position.get('y', 0) / 10) * 10
        return f"{x},{y}"

    def _convert_style_to_css(self, style: Dict) -> str:
        """Convertir un style en CSS"""
        css_parts = []

        if style.get('font'):
            css_parts.append(f"font-family: '{style['font']}', sans-serif")

        if style.get('size'):
            css_parts.append(f"font-size: {style['size']}px")

        if style.get('bold'):
            css_parts.append("font-weight: bold")

        if style.get('italic'):
            css_parts.append("font-style: italic")

        return '; '.join(css_parts) + ';' if css_parts else ''

    def _get_element_z_index(self, element: Dict) -> int:
        """D√©terminer l'ordre de profondeur d'un √©l√©ment"""
        element_type = element.get('type', 'text')

        # Ordre de priorit√©: shapes < images < text < tables
        z_indices = {
            'shape': 1,
            'image': 2,
            'text': 3,
            'table': 4
        }

        return z_indices.get(element_type, 3)

    # M√âTHODES SIMPLIFI√âES POUR LES FONCTIONS NON IMPL√âMENT√âES

    def _extract_with_pdfplumber_advanced(self, file_path: str) -> Dict:
        """Extraction pdfplumber avanc√©e (simplifi√©)"""
        return {}

    def _extract_with_camelot_advanced(self, file_path: str) -> Dict:
        """Extraction camelot avanc√©e (simplifi√©)"""
        return {}

    def _extract_with_tabula_advanced(self, file_path: str) -> Dict:
        """Extraction tabula avanc√©e (simplifi√©)"""
        return {}

    def _extract_with_pixel_analysis(self, file_path: str) -> Dict:
        """Extraction par analyse de pixels (simplifi√©)"""
        return {}

    def _analyze_fonts_and_colors(self, page_dict: Dict, extraction: Dict):
        """Analyser les polices et couleurs (simplifi√©)"""
        pass

    # Autres m√©thodes simplifi√©es...
    def _analyze_grid_patterns(self, lines: List[Dict]) -> List[Dict]:
        return []

    def _is_valid_table_pattern(self, pattern: Dict) -> bool:
        return False

    def _create_table_from_grid_pattern(self, pattern: Dict, page_width: float, page_height: float) -> Optional[Dict]:
        return None

    def _group_text_blocks_by_y(self, text_blocks: List[Dict]) -> List[List[Dict]]:
        return []

    def _is_table_row_candidate(self, y_group: List[Dict]) -> bool:
        return False

    def _find_aligned_rows(self, y_group: List[Dict], y_groups: List[List[Dict]]) -> List[List[Dict]]:
        return []

    def _create_table_from_aligned_text(self, table_rows: List[List[Dict]], page_width: float, page_height: float) -> \
    Optional[Dict]:
        return None

    def _detect_layout_regions(self, image: np.ndarray) -> List[Dict]:
        return []

    def _detect_text_regions(self, image: np.ndarray) -> List[Dict]:
        return []

    def _detect_image_regions(self, image: np.ndarray) -> List[Dict]:
        return []

    def _find_grid_intersections(self, horizontal_lines: List, vertical_lines: List) -> List[Dict]:
        return []

    def _is_valid_table_grid(self, grid: Dict) -> bool:
        return False

    def _create_table_from_visual_grid(self, grid: Dict, image_shape: Tuple) -> Dict:
        return {}

    def _detect_tables_by_template_matching(self, gray_image: np.ndarray) -> List[Dict]:
        return []

    def _detect_tables_by_texture_analysis(self, gray_image: np.ndarray) -> List[Dict]:
        return []

    def _analyze_spatial_relations(self, elements: List[Dict]) -> Dict:
        return {}

    def _ml_element_classification(self, elements: List[Dict]) -> Dict:
        return {}

    def _validate_elements_by_consensus(self, structure_analysis: Dict) -> List[Dict]:
        return structure_analysis.get('all_elements', [])

    def _reconstruct_tables_intelligently(self, elements: List[Dict]) -> List[Dict]:
        return [e for e in elements if e.get('type') == 'table']

    def _reconstruct_content_hierarchy(self, elements: List[Dict]) -> Dict:
        return {}

    def _determine_reading_order(self, elements: List[Dict]) -> List[Dict]:
        return sorted(elements, key=lambda x: (x.get('position', {}).get('y', 0), x.get('position', {}).get('x', 0)))

    def _normalize_whitespace(self, text: str) -> str:
        """Fixe les coupures de mots et retours √† la ligne ind√©sirables.
        - D√©-hyph√©nation: "Detai-\nls" -> "Details"
        - Coupures simples: "Detai\nls" -> "Details"
        - Espace unique entre mots.
        """
        try:
            import re
            if not text:
                return text
            # 1) Coller les mots coup√©s par tiret de fin de ligne
            text = re.sub(r"(\w)-\n(\w)", r"\1\2", text)
            # 2) Coller les mots coup√©s sans tiret (petites lignes)
            text = re.sub(r"(\w)\n(\w)", r"\1 \2", text)
            # 3) Remplacer les multiples espaces/retours par un espace
            text = re.sub(r"[\t\x0b\x0c\r ]+", " ", text)
            # 4) Normaliser doubles sauts en paragraphes
            text = re.sub(r"\n{2,}", "\n\n", text)
            return text.strip()
        except Exception:
            return text

    def _generate_optimized_combined_text(self, elements: List[Dict], reading_order: List[Dict]) -> str:
        text_parts = []
        for element in reading_order:
            if element.get('type') == 'text' and element.get('text'):
                cleaned = self._normalize_whitespace(element['text'])
                text_parts.append(cleaned)
        # Regrouper les paragraphes proprement
        combined = '\n\n'.join(t for t in text_parts if t)
        return self._normalize_whitespace(combined)

    def _validate_table_structure(self, table: Dict) -> Optional[Dict]:
        return table if table.get('data') else None

    def _correct_overlapping_elements(self, elements: List[Dict]) -> List[Dict]:
        return elements

    def _validate_content_hierarchy(self, hierarchy: Dict) -> Dict:
        return hierarchy

    def _calculate_quality_metrics(self, validated: Dict) -> Dict:
        return {
            'elements_count': len(validated.get('validated_elements', [])),
            'tables_count': len(validated.get('tables', [])),
            'avg_confidence': 0.85
        }

    def _calculate_overall_confidence(self, validated: Dict) -> float:
        return 0.85

    # ---- Added minimal safe implementations to avoid missing-attr errors ----
    def _analyze_fonts_and_colors(self, page_dict: Dict, extraction: Dict) -> None:
        try:
            meta = extraction.setdefault('metadata', {})
            fonts = extraction.setdefault('fonts', [])
            colors = extraction.setdefault('colors', [])
            # Minimal aggregation
            for block in page_dict.get('blocks', []):
                if 'lines' in block:
                    for line in block['lines']:
                        for span in line.get('spans', []):
                            if 'size' in span:
                                fonts.append({'size': span.get('size'), 'font': span.get('font')})
                            if 'color' in span:
                                colors.append(span.get('color'))
        except Exception:
            pass

    def _extract_image_with_processing(self, doc, xref: int) -> str:
        try:
            if not PYMUPDF_AVAILABLE:
                return ''
            pix = doc.extract_image(xref)
            img_bytes = pix.get('image', b'')
            ext = pix.get('ext', 'png')
            if not img_bytes:
                return ''
            b64 = base64.b64encode(img_bytes).decode('ascii')
            return f"data:image/{ext};base64,{b64}"
        except Exception as e:
            logger.warning(f"Erreur extraction image: {e}")
            return ''

    def _analyze_image_content(self, image_data: str) -> Dict:
        # Placeholder analysis
        return {'has_content': bool(image_data), 'confidence': 0.8}

    def _merge_overlapping_tables(self, tables: List[Dict]) -> List[Dict]:
        # Minimal: no merge
        return tables or []

    def _validate_and_enrich_table(self, table: Dict, text_blocks: List[Dict]) -> Optional[Dict]:
        # Minimal validation: must have position
        if not isinstance(table, dict):
            return None
        pos = table.get('position') or {}
        if not {'x', 'y', 'width', 'height'}.issubset(pos.keys() if isinstance(pos, dict) else set()):
            return None
        return table

    def _computer_vision_analysis(self, file_path: str) -> Dict:
        # Minimal CV analysis disabled
        return {}

    def _advanced_structure_analysis(self, raw_extractions: Dict, visual_analysis: Dict) -> Dict:
        # Combine available elements from extractions
        all_elements: List[Dict] = []
        for key in ['pymupdf', 'pdfplumber', 'camelot', 'tabula', 'pixel_analysis']:
            data = raw_extractions.get(key) or {}
            els = data.get('elements') or []
            if isinstance(els, list):
                all_elements.extend([e for e in els if isinstance(e, dict)])
        return {'all_elements': all_elements, 'visual': visual_analysis or {}}

    def _generate_ultra_faithful_page_html(self, page_num: int, elements: List[Dict], validated_content: Dict) -> str:
        width = 900
        height = 1200
        parts = [f'<div class="ultra-page-container" data-page="{page_num}" style="width:{width}px;height:{height}px;">']
        for el in elements:
            try:
                pos = el.get('position', {}) if isinstance(el, dict) else {}
                left = pos.get('x_percent') or 0
                top = pos.get('y_percent') or 0
                w = pos.get('width_percent') or 10
                h = pos.get('height_percent') or 10
                style = f'left:{left}%;top:{top}%;width:{w}%;height:{h}%;'
                etype = el.get('type')
                if etype == 'text':
                    parts.append(self._generate_text_element_html(el, style))
                elif etype == 'image':
                    parts.append(self._generate_image_element_html(el, style))
                elif etype == 'table':
                    parts.append(self._generate_table_element_html(el, style))
                else:
                    parts.append(self._generate_shape_element_html(el, style))
            except Exception as e:
                logger.warning(f"Erreur g√©n√©ration √©l√©ment HTML: {e}")
        parts.append('</div>')
        return '\n'.join(parts)

    def _generate_text_element_html(self, el: Dict, style: str) -> str:
        raw = el.get('text', '')
        text = self._normalize_whitespace(raw)
        cls = 'ultra-element ultra-text'
        return f'<div class="{cls}" style="{style}"><div class="editable-content">{text}</div></div>'

    def _generate_image_element_html(self, el: Dict, style: str) -> str:
        src = el.get('image_data') or ''
        cls = 'ultra-element ultra-image'
        if src:
            return f'<div class="{cls}" style="{style}"><img src="{src}" style="width:100%;height:100%;object-fit:contain;"/></div>'
        return f'<div class="{cls}" style="{style}"></div>'

    def _generate_shape_element_html(self, el: Dict, style: str) -> str:
        cls = 'ultra-element ultra-shape'
        return f'<div class="{cls}" style="{style}"></div>'

    def _generate_table_element_html(self, el: Dict, style: str) -> str:
        cls = 'ultra-element ultra-table'
        return f'<div class="{cls}" style="{style}"><table class="ultra-table"></table></div>'

    def _analyze_grid_patterns(self, lines: List[Dict]) -> List[Dict]:
        # Minimal: return empty, avoiding complex grid detection
        return []

    def _is_valid_table_pattern(self, pattern: Dict) -> bool:
        return False